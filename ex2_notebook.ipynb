{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -100,  -100,  -100,  -100, 50264,  -100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.918441772460938"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers \n",
    "import torch\n",
    "import math\n",
    "import random\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = transformers.RobertaForMaskedLM.from_pretrained(\"roberta-base\")\n",
    "\n",
    "inputs = tokenizer(\"I am so <mask>\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of <mask>\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)\n",
    "\n",
    "labels = tokenizer(\"I am so <mask>\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-<mask> tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "print(labels)\n",
    "outputs = model(**inputs, labels=labels)\n",
    "outputs.loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50264\n",
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['Ġam', 'Ġis', \"'m\", 'Ġwas', 'Ġfeel'],\n",
       " [0.9999134540557861,\n",
       "  3.9379392546834424e-05,\n",
       "  2.9938039006083272e-05,\n",
       "  8.689116839377675e-06,\n",
       "  8.550994607503526e-06])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am so <mask>\"\n",
    "top_k = 5\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the logits for masked positions\n",
    "logits = model(input_ids)[\"logits\"]\n",
    "\n",
    "# Find the position of the '<mask>' token in the input\n",
    "mask_token_index = torch.where(input_ids == 524)[1].item()\n",
    "print(tokenizer.mask_token_id)\n",
    "print(mask_token_index)\n",
    "# Get the probabilities for the top-k predictions\n",
    "top_k_values, top_k_indices = torch.topk(logits[0, mask_token_index], top_k)\n",
    "top_k_probabilities = torch.nn.functional.softmax(top_k_values, dim=-1)\n",
    "\n",
    "# Convert indices back to tokens\n",
    "predicted_tokens = tokenizer.convert_ids_to_tokens(top_k_indices.tolist())\n",
    "\n",
    "predicted_tokens, top_k_probabilities.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\liore\\miniconda3\\envs\\nlp_kernel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love similarity\n",
      "[[0.9897104]]\n",
      "bass similarity\n",
      "[[0.86344]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaForMaskedLM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "model_name = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "roberta_model = RobertaModel.from_pretrained(model_name)\n",
    "masked_language_model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "###############1.1###############\n",
    "def get_word_vector(word_index, tokenized_sentence, model):\n",
    "    outputs = model(**tokenized_sentence)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    word_vector = last_hidden_states[0, word_index]\n",
    "    return word_vector\n",
    "\n",
    "###############2###############\n",
    "similar_sentence_1 = 'I love you'\n",
    "similar_sentence_2 = 'I love him'\n",
    "tokenized_similar_sentence_1 = tokenizer(similar_sentence_1, return_tensors='pt')\n",
    "tokenized_similar_sentence_2 = tokenizer(similar_sentence_2, return_tensors='pt')\n",
    "\n",
    "love_1_index = tokenized_similar_sentence_1[\"input_ids\"][0].tolist().index(tokenizer.encode(\" love\")[1])\n",
    "love_2_index = tokenized_similar_sentence_2[\"input_ids\"][0].tolist().index(tokenizer.encode(\" love\")[1])\n",
    "love_1_vector = get_word_vector(love_1_index, tokenized_similar_sentence_1, roberta_model)\n",
    "love_2_vector = get_word_vector(love_2_index, tokenized_similar_sentence_2, roberta_model)\n",
    "\n",
    "print('love similarity')\n",
    "print(cosine_similarity(love_1_vector.detach().numpy().reshape(1, -1), love_2_vector.detach().numpy().reshape(1, -1)))\n",
    "###############3###############\n",
    "different_sentence_1 = 'She decided to clip her hair back with a colorful barrette.'\n",
    "different_sentence_2 = 'Some have come out of nowhere, others have taken months to catch on, and all of them could become ubiquitous in the blink of a TikTok clip.'\n",
    "tokenized_different_sentence_1 = tokenizer(different_sentence_1, return_tensors='pt')\n",
    "tokenized_different_sentence_2 = tokenizer(different_sentence_2, return_tensors='pt')\n",
    "\n",
    "bass_1_index = tokenized_different_sentence_1[\"input_ids\"][0].tolist().index(tokenizer.encode(\" clip\")[1])\n",
    "bass_2_index = tokenized_different_sentence_2[\"input_ids\"][0].tolist().index(tokenizer.encode(\" clip\")[1])\n",
    "bass_1_vector = get_word_vector(bass_1_index, tokenized_different_sentence_1, roberta_model)\n",
    "bass_2_vector = get_word_vector(bass_2_index, tokenized_different_sentence_2, roberta_model)\n",
    "\n",
    "print('bass similarity')\n",
    "print(cosine_similarity(bass_1_vector.detach().numpy().reshape(1, -1), bass_2_vector.detach().numpy().reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bass similarity\n",
      "[[0.8710439]]\n"
     ]
    }
   ],
   "source": [
    "###############3###############\n",
    "similar\n",
    "different_sentence_1 = 'The teacher asked Sarah to lead the class discussion.'\n",
    "different_sentence_2 = 'Be careful not to touch that pencil; it has a lead tip.'\n",
    "tokenized_different_sentence_1 = tokenizer(different_sentence_1, return_tensors='pt')\n",
    "tokenized_different_sentence_2 = tokenizer(different_sentence_2, return_tensors='pt')\n",
    "\n",
    "bass_1_index = tokenized_different_sentence_1[\"input_ids\"][0].tolist().index(tokenizer.encode(\" lead\")[1])\n",
    "bass_2_index = tokenized_different_sentence_2[\"input_ids\"][0].tolist().index(tokenizer.encode(\" lead\")[1])\n",
    "bass_1_vector = get_word_vector(bass_1_index, tokenized_different_sentence_1, roberta_model)\n",
    "bass_2_vector = get_word_vector(bass_2_index, tokenized_different_sentence_2, roberta_model)\n",
    "\n",
    "print('bass similarity')\n",
    "print(cosine_similarity(bass_1_vector.detach().numpy().reshape(1, -1), bass_2_vector.detach().numpy().reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#Returns 2 values: Article dicts: containing the Topic and the article text\n",
    "def get_file_data(file_name, lower = False):\n",
    "    #Each topic is separated by header line, empty line before the text and another empty line\n",
    "    #So we will skip the first two lines, get the thrid line and skip the fourth one \n",
    "    print(\"Reading file\")\n",
    "    with open(file_name, 'r') as file:   \n",
    "        file_data = file.read().splitlines()\n",
    "    \n",
    "    if lower:\n",
    "        file_data = [s.lower() for s in file_data]\n",
    "        \n",
    "    return file_data\n",
    "\n",
    "def get_all_words_no_anotation(data):\n",
    "    words = []\n",
    "    for sen in data:\n",
    "        for word in sen.split(' '):\n",
    "            words.append(word)\n",
    "    \n",
    "    return words\n",
    "\n",
    "def get_all_words_pos(data):\n",
    "    pos_list = []\n",
    "    for sen in data:\n",
    "        for token in sen.split():\n",
    "            word_split = token.rsplit('/', 1)\n",
    "            word_pos = (word_split[0], word_split[1])\n",
    "            pos_list.append(word_pos)\n",
    "    \n",
    "    return pos_list\n",
    "\n",
    "def get_word_freq_per_pos(data):\n",
    "    counts = {}\n",
    "    for word, pos in data:\n",
    "        if word not in counts:\n",
    "            counts[word] = {}\n",
    "\n",
    "        counts[word][pos] = counts[word].get(pos, 0) + 1\n",
    "        \n",
    "    return counts\n",
    "\n",
    "def get_word_most_freq_pos(data):\n",
    "    return {word: max(data[word], key=data[word].get) for word in data}\n",
    "\n",
    "def fill_for_missing_word(data):\n",
    "    all_pos_tags = [pos_tag for pos_dict in data.values() for pos_tag in pos_dict]\n",
    "\n",
    "    return max(set(all_pos_tags), key=all_pos_tags.count)\n",
    "\n",
    "def predict(train_pos_dist_data, test_data, fill_pos_dist, word_sample = False, fill_sample = False):\n",
    "    random.seed(42)\n",
    "    word_pos_pred = []\n",
    "    if not word_sample:\n",
    "        train_word_most_freq_pos = get_word_most_freq_pos(train_pos_dist_data)\n",
    "    if not fill_sample:\n",
    "        fill_value = fill_for_missing_word(train_pos_dist_data)\n",
    "        \n",
    "    for sentence in test_data:\n",
    "        for word in sentence.split():\n",
    "            if word not in train_pos_dist_data:\n",
    "                if fill_sample:\n",
    "                    pos = random.choices(list(fill_pos_dist.keys()), weights = list(fill_pos_dist.values()))[0]\n",
    "                else:\n",
    "                    pos = fill_value\n",
    "\n",
    "                word_pos_pred.append(pos)\n",
    "            else:\n",
    "                if word_sample:\n",
    "                    pos = random.choices(list(train_pos_dist_data[word].keys()), weights = list(train_pos_dist_data[word].values()))[0]\n",
    "                else:\n",
    "                    pos = train_word_most_freq_pos[word]\n",
    "\n",
    "                word_pos_pred.append(pos)\n",
    "    \n",
    "    return word_pos_pred\n",
    "\n",
    "def calc_Accuracy(y_pred, y_true):\n",
    "    if len(y_pred) != len(y_true):\n",
    "        raise ValueError(\"Lists need to be the same length\")\n",
    "    \n",
    "    return sum(p == t for p, t in zip(y_pred, y_true)) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_inflection(train_pos_dist_data, test_data, fill_pos_dist, word_sample = False, fill_sample = False):\n",
    "    random.seed(42)\n",
    "    word_pos_pred = []\n",
    "    if not word_sample:\n",
    "        train_word_most_freq_pos = get_word_most_freq_pos(train_pos_dist_data)\n",
    "    if not fill_sample:\n",
    "        fill_value = fill_for_missing_word(train_pos_dist_data)\n",
    "        \n",
    "    for sentence in test_data:\n",
    "        for word in sentence.split():\n",
    "            if all(w not in train_pos_dist_data for w in [word, word.lower(), word.capitalize()]):\n",
    "                if fill_sample:\n",
    "                    pos = random.choices(list(fill_pos_dist.keys()), weights = list(fill_pos_dist.values()))[0]\n",
    "                else:\n",
    "                    pos = fill_value\n",
    "\n",
    "                word_pos_pred.append(pos)\n",
    "            else:\n",
    "                if word in train_pos_dist_data:\n",
    "                    pass\n",
    "                elif word.lower() in train_pos_dist_data:\n",
    "                    word = word.lower()\n",
    "                elif word.capitalize() in train_pos_dist_data:\n",
    "                    word = word.capitalize()\n",
    "                    \n",
    "                if word_sample:\n",
    "                    pos = random.choices(list(train_pos_dist_data[word].keys()), weights = list(train_pos_dist_data[word].values()))[0]\n",
    "                else:\n",
    "                    pos = train_word_most_freq_pos[word]\n",
    "\n",
    "                word_pos_pred.append(pos)\n",
    "    \n",
    "    return word_pos_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_dict(data):\n",
    "    counts = {}\n",
    "    for sentence in data:\n",
    "        prev_pos = ''\n",
    "        for token in sentence.split():\n",
    "            word_split = token.rsplit('/', 1)\n",
    "            pos_word = f'{prev_pos}_{word_split[0]}'\n",
    "            if pos_word not in counts:\n",
    "                counts[pos_word] = {}\n",
    "\n",
    "            counts[pos_word][word_split[1]] = counts[pos_word].get(word_split[1], 0) + 1\n",
    "            prev_pos = word_split[1]\n",
    "            \n",
    "    return counts\n",
    "\n",
    "def bigram_predict(train_pos_word_dist_data, train_pos_dist_data, test_data, fill_pos_dist, word_sample = False, fill_sample = False):\n",
    "    random.seed(42)\n",
    "    word_pos_pred = []\n",
    "    train_pos_word_most_freq_pos = get_word_most_freq_pos(train_pos_word_dist_data)\n",
    "    if not word_sample:\n",
    "        train_word_most_freq_pos = get_word_most_freq_pos(train_pos_dist_data)\n",
    "    if not fill_sample:\n",
    "        fill_value = fill_for_missing_word(train_pos_dist_data)\n",
    "        \n",
    "    for sentence in test_data:\n",
    "        prev_pos = ''\n",
    "        for word in sentence.split():\n",
    "            if f'{prev_pos}_{word}' not in train_pos_word_dist_data:\n",
    "                if word not in train_pos_dist_data:\n",
    "                    pos = fill_value\n",
    "                else:\n",
    "                    pos = train_word_most_freq_pos[word]\n",
    "            else:\n",
    "                pos = train_pos_word_most_freq_pos[f'{prev_pos}_{word}']\n",
    "\n",
    "            word_pos_pred.append(pos)\n",
    "            prev_pos = pos\n",
    "    \n",
    "    return word_pos_pred\n",
    "\n",
    "def bigram_predict_with_inflection(train_pos_word_dist_data, train_pos_dist_data, test_data, fill_pos_dist, word_sample = False, fill_sample = False):\n",
    "    random.seed(42)\n",
    "    word_pos_pred = []\n",
    "    train_pos_word_most_freq_pos = get_word_most_freq_pos(train_pos_word_dist_data)\n",
    "    if not word_sample:\n",
    "        train_word_most_freq_pos = get_word_most_freq_pos(train_pos_dist_data)\n",
    "    if not fill_sample:\n",
    "        fill_value = fill_for_missing_word(train_pos_dist_data)\n",
    "        \n",
    "    for sentence in test_data:\n",
    "        prev_pos = ''\n",
    "        for word in sentence.split():\n",
    "            if all(w not in train_pos_word_dist_data for w in [f'{prev_pos}_{word}',\n",
    "                                                          f'{prev_pos}_{word.lower()}',\n",
    "                                                          f'{prev_pos}_{word.capitalize()}']):\n",
    "                if all(w not in train_pos_dist_data for w in [word, word.lower(), word.capitalize()]):\n",
    "                    pos = fill_value\n",
    "                else:\n",
    "                    if word in train_pos_dist_data:\n",
    "                        pass\n",
    "                    elif word.lower() in train_pos_dist_data:\n",
    "                        word = word.lower()\n",
    "                    elif word.capitalize() in train_pos_dist_data:\n",
    "                        word = word.capitalize()\n",
    "                    pos = train_word_most_freq_pos[word]\n",
    "            else:\n",
    "                if f'{prev_pos}_{word}' in train_pos_word_dist_data:\n",
    "                    pass\n",
    "                elif f'{prev_pos}_{word.lower()}' in train_pos_word_dist_data:\n",
    "                    word = word.lower()\n",
    "                elif f'{prev_pos}_{word.capitalize()}' in train_pos_word_dist_data:\n",
    "                    word = word.capitalize()\n",
    "                pos = train_pos_word_most_freq_pos[f'{prev_pos}_{word}']\n",
    "\n",
    "            word_pos_pred.append(pos)\n",
    "            prev_pos = pos\n",
    "    \n",
    "    return word_pos_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n"
     ]
    }
   ],
   "source": [
    "train_file = './data/pos/ass1-tagger-train'\n",
    "train_sentences = get_file_data(train_file)\n",
    "words_pos = get_all_words_pos(train_sentences)\n",
    "train_data_freq = get_word_freq_per_pos(words_pos)\n",
    "train_data_mdl = get_word_most_freq_pos(train_data_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Reading file\n",
      "bigram prediction:  0.9291606836162964\n"
     ]
    }
   ],
   "source": [
    "dev_input_file = './data/pos/ass1-tagger-dev-input'\n",
    "dev_file = './data/pos/ass1-tagger-dev'\n",
    "dev_sentences = get_file_data(dev_file)\n",
    "dev_actual = [pos for word, pos in get_all_words_pos(dev_sentences)]\n",
    "train_pos_word_freq = get_bigram_dict(train_sentences)\n",
    "dev_input_sentences = get_file_data(dev_input_file)\n",
    "train_pos_freq_dict = Counter([v for k, v in words_pos])\n",
    "dev_predict_bigrm = bigram_predict(train_pos_word_freq, train_data_freq, dev_input_sentences, train_pos_freq_dict)\n",
    "print('bigram prediction: ', calc_Accuracy(dev_predict_bigrm, dev_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigram with inflection prediction:  0.9271897964355098\n"
     ]
    }
   ],
   "source": [
    "dev_predict_bigrm_inflection = bigram_predict_with_inflection(train_pos_word_freq, train_data_freq, dev_input_sentences, train_pos_freq_dict)\n",
    "print('bigram with inflection prediction: ', calc_Accuracy(dev_predict_bigrm_inflection, dev_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      "Reading file\n",
      "Reading file\n",
      "predictions with every possibilty of sampling:\n",
      "max word and max fill:  0.911591632176141\n",
      "max word and sample fill:  0.9042430385449222\n",
      "sample word and max fill:  0.8902497395613368\n",
      "sample word and sample fill:  0.8836331897401245\n"
     ]
    }
   ],
   "source": [
    "dev_input_file = './data/pos/ass1-tagger-dev-input'\n",
    "dev_file = './data/pos/ass1-tagger-dev'\n",
    "dev_input_sentences = get_file_data(dev_input_file)\n",
    "dev_sentences = get_file_data(dev_file)\n",
    "dev_actual = [pos for word, pos in get_all_words_pos(dev_sentences)]\n",
    "train_pos_freq_dict = Counter([v for k, v in words_pos])\n",
    "sentences = get_file_data(dev_input_file)\n",
    "dev_predict_f_f = predict(train_data_freq, dev_input_sentences, train_pos_freq_dict, word_sample = False, fill_sample = False)\n",
    "dev_predict_f_t = predict(train_data_freq, dev_input_sentences, train_pos_freq_dict, word_sample = False, fill_sample = True)\n",
    "dev_predict_t_f = predict(train_data_freq, dev_input_sentences, train_pos_freq_dict, word_sample = True, fill_sample = False)\n",
    "dev_predict_t_t = predict(train_data_freq, dev_input_sentences, train_pos_freq_dict, word_sample = True, fill_sample = True)\n",
    "print('predictions with every possibilty of sampling:')\n",
    "print('max word and max fill: ', calc_Accuracy(dev_predict_f_f, dev_actual))\n",
    "print('max word and sample fill: ', calc_Accuracy(dev_predict_f_t, dev_actual))\n",
    "print('sample word and max fill: ', calc_Accuracy(dev_predict_t_f, dev_actual))\n",
    "print('sample word and sample fill: ', calc_Accuracy(dev_predict_t_t, dev_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions with inflection with every possibilty of sampling:\n",
      "max word and max fill:  0.912182898330377\n",
      "max word and sample fill:  0.9055381929780105\n",
      "sample word and max fill:  0.8926711152405891\n",
      "sample word and sample fill:  0.8849283441732129\n"
     ]
    }
   ],
   "source": [
    "dev_predict_f_f_inflection = predict_with_inflection(train_data_freq, dev_input_sentences, train_pos_freq_dict, word_sample = False, fill_sample = False)\n",
    "dev_predict_f_t_inflection = predict_with_inflection(train_data_freq, dev_input_sentences, train_pos_freq_dict, word_sample = False, fill_sample = True)\n",
    "dev_predict_t_f_inflection = predict_with_inflection(train_data_freq, dev_input_sentences, train_pos_freq_dict, word_sample = True, fill_sample = False)\n",
    "dev_predict_t_t_inflection = predict_with_inflection(train_data_freq, dev_input_sentences, train_pos_freq_dict, word_sample = True, fill_sample = True)\n",
    "print('predictions with inflection with every possibilty of sampling:')\n",
    "print('max word and max fill: ', calc_Accuracy(dev_predict_f_f_inflection, dev_actual))\n",
    "print('max word and sample fill: ', calc_Accuracy(dev_predict_f_t_inflection, dev_actual))\n",
    "print('sample word and max fill: ', calc_Accuracy(dev_predict_t_f_inflection, dev_actual))\n",
    "print('sample word and sample fill: ', calc_Accuracy(dev_predict_t_t_inflection, dev_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
